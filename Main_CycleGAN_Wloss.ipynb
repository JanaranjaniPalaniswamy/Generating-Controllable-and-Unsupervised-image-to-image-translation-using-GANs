{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from WeightDecay import WeightDecay\n",
    "import matplotlib.pyplot as plt\n",
    "import imlib as im\n",
    "import numpy as np\n",
    "import pylib as py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tf2lib as tl\n",
    "import tf2gan as gan\n",
    "import tqdm\n",
    "import glob\n",
    "import data\n",
    "import module\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy.random import random\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "load_size = 72\n",
    "crop_size = 64\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "len_dataset = 30000\n",
    "epoch_decay = epochs // 2\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "adversarial_loss_mode = 'wgan'\n",
    "gradient_penalty_mode = 'wgan-gp'\n",
    "gradient_penalty_weight = 10\n",
    "cycle_loss_weight = 10\n",
    "identity_loss_weight = 5\n",
    "pool_size = 50\n",
    "output_dir = \"/home/palaniswamyji/Baseline/ws2020_janaranjanikoushik/\"\n",
    "G_losses = []\n",
    "D_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parse_func(filename):\n",
    "    # Read the image\n",
    "    image_string = tf.io.read_file(filename)\n",
    "\n",
    "    # Decode the image\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "    # Normalize the image\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "    image = ((image / 127.5) - 1)\n",
    "\n",
    "    # resizing to 72 x 72 x 3\n",
    "    image = tf.image.resize(image, [load_size, load_size],\n",
    "                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    # random crop to 64 x 64 x 3\n",
    "    image = tf.image.random_crop(image, size=[crop_size, crop_size, 3])\n",
    "    \n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parse_func(filename):\n",
    "    # Read the image\n",
    "    image_string = tf.io.read_file(filename)\n",
    "\n",
    "    # Decode the image\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "    # Normalize the image\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "    image = ((image / 127.5) - 1)\n",
    "\n",
    "    # resizing to 64 x 64 x 3\n",
    "    image = tf.image.resize(image, [crop_size, crop_size],\n",
    "                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset from file\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "dataset_real = glob.glob(\"/home/palaniswamyji/celebA_ds/*\")\n",
    "dataset_comic = glob.glob(\"/home/palaniswamyji/facemaker_ds/*\")\n",
    "\n",
    "# Count of dataset\n",
    "dataset_real_count = 30000\n",
    "dataset_comic_count = 30000\n",
    "\n",
    "# Training count\n",
    "dataset_real_training_count = int(dataset_real_count * 0.8)\n",
    "dataset_comic_training_count = int(dataset_comic_count * 0.8)\n",
    "\n",
    "# Split training and test dataset\n",
    "train_real = tf.data.Dataset.from_tensor_slices(dataset_real[0:dataset_real_training_count])\n",
    "test_real = tf.data.Dataset.from_tensor_slices(dataset_real[dataset_real_training_count:dataset_real_count])\n",
    "train_comic = tf.data.Dataset.from_tensor_slices(dataset_comic[0:dataset_comic_training_count])\n",
    "test_comic = tf.data.Dataset.from_tensor_slices(dataset_comic[dataset_comic_training_count:dataset_comic_count])\n",
    "\n",
    "A2B_pool = data.ItemPool(pool_size)\n",
    "B2A_pool = data.ItemPool(pool_size)\n",
    "\n",
    "train_comic = train_comic.map(\n",
    "    train_parse_func, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(batch_size)\n",
    "\n",
    "test_comic = test_comic.map(\n",
    "    test_parse_func, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(batch_size)\n",
    "\n",
    "train_real = train_real.map(\n",
    "    train_parse_func, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(batch_size)\n",
    "\n",
    "test_real = test_real.map(\n",
    "    test_parse_func, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(batch_size)\n",
    "\n",
    "A_B_dataset = tf.data.Dataset.zip((train_comic, train_real))\n",
    "A_B_dataset_test = tf.data.Dataset.zip((test_comic, test_real))\n",
    "\n",
    "len_dataset = dataset_real_training_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_A2B = module.ResnetGenerator(input_shape=(crop_size, crop_size, 3))\n",
    "G_B2A = module.ResnetGenerator(input_shape=(crop_size, crop_size, 3))\n",
    "\n",
    "D_A = module.ConvDiscriminator(input_shape=(crop_size, crop_size, 3))\n",
    "D_B = module.ConvDiscriminator(input_shape=(crop_size, crop_size, 3))\n",
    "\n",
    "inceptionModel = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet', input_tensor=None, input_shape=(75, 75, 3), pooling=None, classes=1000, classifier_activation='softmax')\n",
    "\n",
    "d_loss_fn, g_loss_fn = gan.get_adversarial_losses_fn(adversarial_loss_mode)\n",
    "cycle_loss_fn = tf.losses.MeanAbsoluteError()\n",
    "identity_loss_fn = tf.losses.MeanAbsoluteError()\n",
    "\n",
    "G_lr_scheduler = module.LinearDecay(lr, epochs * len_dataset, epoch_decay * len_dataset)\n",
    "D_lr_scheduler = module.LinearDecay(lr, epochs * len_dataset, epoch_decay * len_dataset)\n",
    "G_optimizer = keras.optimizers.Adam(learning_rate=G_lr_scheduler, beta_1=beta_1)\n",
    "D_optimizer = keras.optimizers.Adam(learning_rate=D_lr_scheduler, beta_1=beta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageToFeatureVector(image):\n",
    "    image = tf.image.resize(image, [75, 75], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    image = tf.reshape(image, (1, 75, 75, 3), name=None)\n",
    "    return inceptionModel(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_G(A, B):\n",
    "    with tf.GradientTape() as t:\n",
    "        A2B = G_A2B(A, training=True)\n",
    "        B2A = G_B2A(B, training=True)\n",
    "        A2B2A = G_B2A(A2B, training=True)\n",
    "        B2A2B = G_A2B(B2A, training=True)\n",
    "        A2A = G_B2A(A, training=True)\n",
    "        B2B = G_A2B(B, training=True)\n",
    "\n",
    "        A2B_d_logits = D_B(A2B, training=True)\n",
    "        B2A_d_logits = D_A(B2A, training=True)\n",
    "\n",
    "        A2B_g_loss = g_loss_fn(A2B_d_logits)\n",
    "        B2A_g_loss = g_loss_fn(B2A_d_logits)\n",
    "        \n",
    "        A2B2A_cycle_loss = cycle_loss_fn(A, A2B2A)\n",
    "        B2A2B_cycle_loss = cycle_loss_fn(B, B2A2B)\n",
    "        A2A_id_loss = identity_loss_fn(A, A2A)\n",
    "        B2B_id_loss = identity_loss_fn(B, B2B)\n",
    "\n",
    "        G_loss = (A2B_g_loss + B2A_g_loss) + (A2B2A_cycle_loss + B2A2B_cycle_loss) * cycle_loss_weight + (A2A_id_loss + B2B_id_loss) * identity_loss_weight\n",
    "        \n",
    "    G_grad = t.gradient(G_loss, G_A2B.trainable_variables + G_B2A.trainable_variables)\n",
    "    G_optimizer.apply_gradients(zip(G_grad, G_A2B.trainable_variables + G_B2A.trainable_variables))\n",
    "\n",
    "    return A2B, B2A, G_loss, {'A2B_g_loss': A2B_g_loss,\n",
    "                              'B2A_g_loss': B2A_g_loss,\n",
    "                              'A2B2A_cycle_loss': A2B2A_cycle_loss,\n",
    "                              'B2A2B_cycle_loss': B2A2B_cycle_loss,\n",
    "                              'A2A_id_loss': A2A_id_loss,\n",
    "                              'B2B_id_loss': B2B_id_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_D(A, B, A2B, B2A):\n",
    "    with tf.GradientTape() as t:\n",
    "        A_d_logits = D_A(A, training=True)\n",
    "        B2A_d_logits = D_A(B2A, training=True)\n",
    "        B_d_logits = D_B(B, training=True)\n",
    "        A2B_d_logits = D_B(A2B, training=True)\n",
    "\n",
    "        A_d_loss, B2A_d_loss = d_loss_fn(A_d_logits, B2A_d_logits)\n",
    "        B_d_loss, A2B_d_loss = d_loss_fn(B_d_logits, A2B_d_logits)\n",
    "        D_A_gp = gan.gradient_penalty(functools.partial(D_A, training=True), A, B2A, mode=gradient_penalty_mode)\n",
    "        D_B_gp = gan.gradient_penalty(functools.partial(D_B, training=True), B, A2B, mode=gradient_penalty_mode)\n",
    "\n",
    "        D_loss = (A_d_loss + B2A_d_loss) + (B_d_loss + A2B_d_loss) + (D_A_gp + D_B_gp) * gradient_penalty_weight\n",
    "\n",
    "    D_grad = t.gradient(D_loss, D_A.trainable_variables + D_B.trainable_variables)\n",
    "    D_optimizer.apply_gradients(zip(D_grad, D_A.trainable_variables + D_B.trainable_variables))\n",
    "\n",
    "    return D_loss, {'A_d_loss': A_d_loss + B2A_d_loss,\n",
    "                    'B_d_loss': B_d_loss + A2B_d_loss,\n",
    "                    'D_A_gp': D_A_gp,\n",
    "                    'D_B_gp': D_B_gp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(A, B):\n",
    "    A2B, B2A, G_loss, G_loss_dict = train_G(A, B)\n",
    "\n",
    "    # cannot autograph `A2B_pool`\n",
    "    A2B = A2B_pool(A2B)  # or A2B = A2B_pool(A2B.numpy()), but it is much slower\n",
    "    B2A = B2A_pool(B2A)  # because of the communication between CPU and GPU\n",
    "\n",
    "    D_loss, D_loss_dict = train_D(A, B, A2B, B2A)\n",
    "    \n",
    "    return G_loss_dict, D_loss_dict, G_loss, D_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sample(A, B):\n",
    "    A2B = G_A2B(A, training=False)\n",
    "    B2A = G_B2A(B, training=False)\n",
    "    A2B2A = G_B2A(A2B, training=False)\n",
    "    B2A2B = G_A2B(B2A, training=False)\n",
    "    return A2B, B2A, A2B2A, B2A2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        plt.title(title[i])\n",
    "\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch counter\n",
    "ep_cnt = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = tl.Checkpoint(dict(G_A2B=G_A2B,\n",
    "                                G_B2A=G_B2A,\n",
    "                                D_A=D_A,\n",
    "                                D_B=D_B,\n",
    "                                G_optimizer=G_optimizer,\n",
    "                                D_optimizer=D_optimizer,\n",
    "                                G_losses=G_losses,\n",
    "                                D_losses=D_losses,\n",
    "                                ep_cnt=ep_cnt),\n",
    "                           py.join(output_dir, 'checkpoints_CycleGAN_Wloss'),\n",
    "                           max_to_keep=5)\n",
    "try:  # restore checkpoint including the epoch counter\n",
    "    checkpoint.restore().assert_existing_objects_matched()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# summary\n",
    "train_summary_writer = tf.summary.create_file_writer(py.join(output_dir, 'summaries_CycleGAN_Wloss', 'train'))\n",
    "\n",
    "# sample\n",
    "test_iter = iter(A_B_dataset_test)\n",
    "train_iter = iter(A_B_dataset)\n",
    "sample_dir = py.join(output_dir, 'samples_training_CycleGAN_Wloss')\n",
    "py.mkdir(sample_dir)\n",
    "\n",
    "# main loop\n",
    "with train_summary_writer.as_default():\n",
    "    for ep in tqdm.trange(epochs, desc='Epoch Loop'):\n",
    "        if ep < ep_cnt:\n",
    "            continue\n",
    "\n",
    "        # update epoch counter\n",
    "        ep_cnt.assign_add(1)\n",
    "\n",
    "        # train for an epoch\n",
    "        for A, B in tqdm.tqdm(A_B_dataset, desc='Inner Epoch Loop', total=len_dataset):\n",
    "            G_loss_dict, D_loss_dict, G_loss, D_loss = train_step(A, B)\n",
    "            G_losses.append(G_loss)\n",
    "            D_losses.append(D_loss)\n",
    "\n",
    "            # # summary\n",
    "            tl.summary(G_loss_dict, step=G_optimizer.iterations, name='G_losses')\n",
    "            tl.summary(D_loss_dict, step=G_optimizer.iterations, name='D_losses')\n",
    "            tl.summary({'learning rate': G_lr_scheduler.current_learning_rate}, step=G_optimizer.iterations, name='learning rate')\n",
    "\n",
    "            # sample\n",
    "            if G_optimizer.iterations.numpy() % 100 == 0:\n",
    "                A, B = next(test_iter)\n",
    "                A2B, B2A, A2B2A, B2A2B = sample(A, B)\n",
    "                img = im.immerge(np.concatenate([A, A2B, A2B2A, B, B2A, B2A2B], axis=0), n_rows=2)\n",
    "                im.imwrite(img, py.join(sample_dir, 'iter-%09d.jpg' % G_optimizer.iterations.numpy()))\n",
    "    \n",
    "        A, B = next(train_iter)\n",
    "        generate_images(G_A2B, A)\n",
    "        generate_images(G_B2A, G_A2B(A))\n",
    "        # save checkpoint\n",
    "        checkpoint.save(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot loss values\n",
    "tl.Checkpoint(dict(G_losses=G_losses, D_losses=D_losses), py.join(output_dir, 'checkpoints_CycleGAN_LS_loss')).restore()\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "plt.plot(G_losses, label=\"Total Generator Loss\", lw=2)\n",
    "plt.plot(D_losses, label=\"Total Discriminator Loss\", lw=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"batch\", fontsize=20)\n",
    "plt.ylabel(\"Loss\",  fontsize=20)\n",
    "plt.legend(fontsize=20, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.Checkpoint(dict(G_A2B=G_A2B, G_B2A=G_B2A), py.join(output_dir, 'checkpoints_CycleGAN_Wloss')).restore()\n",
    "\n",
    "@tf.function\n",
    "def sample_A2B(A):\n",
    "    A2B = G_A2B(A, training=False)\n",
    "    A2B2A = G_B2A(A2B, training=False)\n",
    "    return A2B, A2B2A\n",
    "\n",
    "@tf.function\n",
    "def sample_B2A(B):\n",
    "    B2A = G_B2A(B, training=False)\n",
    "    B2A2B = G_A2B(B2A, training=False)\n",
    "    return B2A, B2A2B\n",
    "\n",
    "# run\n",
    "save_dir = py.join(output_dir, 'samples_testing_CycleGAN_Wloss', 'A2B')\n",
    "py.mkdir(save_dir)\n",
    "i = 0\n",
    "for A in test_comic:\n",
    "    A2B, A2B2A = sample_A2B(A)\n",
    "    for A_i, A2B_i, A2B2A_i in zip(A, A2B, A2B2A):\n",
    "        img = np.concatenate([A_i.numpy(), A2B_i.numpy(), A2B2A_i.numpy()], axis=1)\n",
    "        im.imwrite(img, py.join(save_dir, 'iter-%09d.jpg' % i))\n",
    "        i += 1\n",
    "\n",
    "save_dir = py.join(output_dir, 'samples_testing_CycleGAN_Wloss', 'B2A')\n",
    "py.mkdir(save_dir)\n",
    "i = 0\n",
    "for B in train_real:\n",
    "    B2A, B2A2B = sample_B2A(B)\n",
    "    for B_i, B2A_i, B2A2B_i in zip(B, B2A, B2A2B):\n",
    "        img = np.concatenate([B_i.numpy(), B2A_i.numpy(), B2A2B_i.numpy()], axis=1)\n",
    "        im.imwrite(img, py.join(save_dir, 'iter-%09d.jpg' % i))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Evaluation\n",
    "test_length = batch_size * 20\n",
    "sample_test_comic = tf.data.Dataset.from_tensor_slices(dataset_comic[0:test_length])\n",
    "sample_test_comic = sample_test_comic.map(test_parse_func, num_parallel_calls=AUTOTUNE).cache().batch(batch_size)\n",
    "\n",
    "iterator = iter(sample_test_comic)\n",
    "for i in range(20):\n",
    "    generate_images(G_A2B, next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID evaluation\n",
    "def convertImagesToActivationArray(dataset, toGenerate):\n",
    "    activation_list = []\n",
    "    for images in dataset:\n",
    "        if toGenerate:\n",
    "            images = G_A2B(images)\n",
    "        for image in images:\n",
    "            activation = tf.reshape(imageToFeatureVector(image), (2048), name=None)\n",
    "            activation_list.append(activation)\n",
    "    return np.array(activation_list)\n",
    "\n",
    "def getFid(activations1, activations2):\n",
    "    mu1, sigma1 = activations1.mean(axis=0), cov(activations1, rowvar=False)\n",
    "    mu2, sigma2 = activations2.mean(axis=0), cov(activations2, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "print(\"real data set to real numpy\")\n",
    "actications_real_numpy = convertImagesToActivationArray(test_real, False)\n",
    "\n",
    "print(\"comic data set to real numpy\")\n",
    "actications_comic_numpy = convertImagesToActivationArray(test_comic, False)\n",
    "\n",
    "print(\"generated data set to real numpy\")\n",
    "actications_generated_numpy = convertImagesToActivationArray(test_comic, True)\n",
    "\n",
    "print(\"FID from real and comic data set : \")\n",
    "print(getFid(actications_real_numpy, actications_comic_numpy))\n",
    "\n",
    "print(\"FID from real and generated data set : \")\n",
    "print(getFid(actications_real_numpy, actications_generated_numpy))\n",
    "\n",
    "print(\"FID from comic and generated data set : \")\n",
    "print(getFid(actications_comic_numpy, actications_generated_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-crack",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
